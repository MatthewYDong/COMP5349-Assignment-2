{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532fa39d",
   "metadata": {},
   "source": [
    "https://ieeexplore.ieee.org/document/7072954\n",
    "TDIDF cosine similarity\n",
    "\n",
    "https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630\n",
    "Calculating document similarities\n",
    "\n",
    "https://edstem.org/courses/5492/discussion/469433\n",
    "Usually would be using feature extraction on tweets themselves, but for sake of simplicity we are doing it on the document represention instead.\n",
    "\n",
    "https://edstem.org/courses/5492/discussion/471511\n",
    "NB: USE STRING REPRESENTATIONS OF THE DOCUMENT REPRESENTATION; (539, 47, 4) == \"539 47 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb81e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Word2Vec\n",
    "import random\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78afe4",
   "metadata": {},
   "source": [
    "## Workload 1\n",
    "\n",
    "### Data-Loading and Creating Document Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba6966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- hash_tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- replyto_id: long (nullable = true)\n",
      " |-- replyto_user_id: long (nullable = true)\n",
      " |-- retweet_id: long (nullable = true)\n",
      " |-- retweet_user_id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_mentions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Begin Spark session.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"mdon9995 Assignment 2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load data from file.\n",
    "file_path = \"tweets.json\"\n",
    "tweets_data_raw = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "\n",
    "# Show schema for later reference.\n",
    "tweets_data_raw.printSchema()\n",
    "\n",
    "# Create temporary view for SQL.\n",
    "tweets_data_raw.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7036143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 8833 entries from json file.\n",
      "\n",
      "\n",
      "Grouped into 8223 entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Retrieve user, reply and retweet ids per entry.\n",
    "# By using concat_ws we also remove the NULLS that occur.\n",
    "df = spark.sql(\"\"\"\n",
    "                    SELECT user_id, CONCAT_WS(\" \", replyto_id, retweet_id) AS reply_retweet\n",
    "                    FROM tweets\n",
    "                    WHERE replyto_id IS NOT NULL\n",
    "                    OR retweet_id IS NOT NULL\n",
    "                    \"\"\")\n",
    "# df.show(20, False)\n",
    "print(\"Extracted {} entries from json file.\".format(df.count()))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Grouping by user_id, aggregate by concatenating across entries, with final column name of doc_rep\n",
    "df_group = df.groupby(\"user_id\")\\\n",
    "            .agg(f.concat_ws(\" \", f.collect_list(df.reply_retweet))\\\n",
    "            .alias(\"doc_rep\")).cache()\n",
    "# df_group.show(20, False)\n",
    "print(\"Grouped into {} entries.\".format(df_group.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d3cfa",
   "metadata": {},
   "source": [
    "### Generate an input user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4629b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated user id: 1205906586734125056\n"
     ]
    }
   ],
   "source": [
    "# I recognise this takes a few extra computational steps, but generally user_id would be predetermined and wouldn't take time to run.\n",
    "\n",
    "\n",
    "random.seed(430113983)\n",
    "user_id = random.choice(df_group.select(\"user_id\").collect())[0]\n",
    "print('Randomly generated user id: {}'.format(user_id))\n",
    "\n",
    "# Take input user_id and return its features.\n",
    "def get_user(user_id, data, features):\n",
    "    user_features = data.filter(data.user_id == user_id).select(\"user_id\", features)\n",
    "    return user_features\n",
    "\n",
    "# Define cosine similarity function\n",
    "def cosine(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9834e56",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1aff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input 'sentences' to individual 'words'.\n",
    "tokenizer = Tokenizer(inputCol=\"doc_rep\", outputCol=\"id\")\n",
    "data_id = tokenizer.transform(df_group)\n",
    "\n",
    "# Can introduce more for testing, but at a cost to computation. It seemed 20 is suggested as a default and it worked fine.\n",
    "numFeatures = 50\n",
    "\n",
    "# Create hashing table for term frequency count.\n",
    "hashingTF = HashingTF(inputCol=\"id\", outputCol=\"rawFeatures\", numFeatures=numFeatures)\n",
    "data_featurized = hashingTF.transform(data_id)\n",
    "data_featurized.cache() # We will call this twice for TF-IDF\n",
    "\n",
    "# Calculating IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(data_featurized)\n",
    "data_rescaled = idf_model.transform(data_featurized)\n",
    "\n",
    "# Run above function, extract features only.\n",
    "input_user_features = get_user(user_id, data_rescaled, \"features\").collect()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d12e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out input user from comparisons, then conver to rdd for mapping.\n",
    "# I think udfs would work better here, but I was unable to implement due to time constraints.\n",
    "v = data_rescaled.select(\"user_id\", \"features\").filter(data_rescaled.user_id != user_id).rdd\n",
    "\n",
    "# Computing cosine similarity between input user's features and the entire dataset.\n",
    "similarities = v.map(lambda x: (x[0], cosine(input_user_features, x[1])))\n",
    "\n",
    "# Sorting above.\n",
    "top_five = similarities.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collect here due to multiple calls soon for printing output.\n",
    "top_five_list = top_five.collect()\n",
    "\n",
    "# I noticed a lot of tweets having the same similarities of 1, so the code below counts the amount.\n",
    "# Extracting the fifth highest similarity that was included. Any similarities that are the same should thus also be included.\n",
    "# [4] refers to the above, while [1] extracts the similarity score from the fifth most similar word.\n",
    "top_similarity = top_five_list[4][1]\n",
    "top_one_same = top_five.filter(lambda x: x[1] == top_similarity).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01751e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TD-IDF for feature extraction, the top 5 users with similar interests were:\n",
      "user_id: 302864870, similarity score: 1.000\n",
      "user_id: 1214658729938997253, similarity score: 1.000\n",
      "user_id: 1382024523097239556, similarity score: 1.000\n",
      "user_id: 770655466590445568, similarity score: 1.000\n",
      "user_id: 25576953, similarity score: 1.000\n",
      "There were also 189 omitted entries with a similarity score of 1.000\n"
     ]
    }
   ],
   "source": [
    "# Returning results for TDIDF.\n",
    "print('Using TD-IDF for feature extraction, the top 5 users with similar interests were:')\n",
    "\n",
    "for i in range(5):\n",
    "    print('user_id: {}, similarity score: {:.3f}'.format(top_five_list[i][0], top_five_list[i][1]))\n",
    "\n",
    "if len(top_one_same) > 5:\n",
    "    print('There were also {} omitted entries with a similarity score of {:.3f}'.format(len(top_one_same)-5, top_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d4dda",
   "metadata": {},
   "source": [
    "### Word2Vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee5db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# type(doc_rep) = string, needed array<string> as per Word2Vec requirements\n",
    "df_w2v = df_group.withColumn('split', split(df_group[\"doc_rep\"], \" \")).drop(\"doc_rep\")\n",
    "\n",
    "# Initialise word2vec model, I found having a larger vector size had an effect on the amount of similar vectors to a given user_id\n",
    "# Therefore I kept it fairly large, as it did not have a noticeable effect on runtime on my machine.\n",
    "word2vec = Word2Vec(vectorSize=50, minCount=0, inputCol=\"split\", outputCol=\"result\")\n",
    "\n",
    "# Fit and Transform model on prepared document representation (with split)\n",
    "model = word2vec.fit(df_w2v)\n",
    "result = model.transform(df_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5a4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning features for input user_id.\n",
    "# [0][\"result\"] is for extraction from dataframe purposes.\n",
    "input_user_features_w2v = get_user(user_id, result, \"result\")\\\n",
    "                            .collect()[0][\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579849be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out input user, then rdd.map cosine similarity\n",
    "similarities_w2v = result.filter(result.user_id != user_id).rdd.map(lambda x: (x[0], cosine(input_user_features_w2v, x[2])))\n",
    "\n",
    "# Sorting above.\n",
    "top_five_w2v = similarities_w2v.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collect here due to multiple calls soon for printing output.\n",
    "top_five_list_w2v = top_five_w2v.collect()\n",
    "\n",
    "# I noticed a lot of tweets having the same similarities of 1, so the code below counts the amount.\n",
    "# Extracting the fifth highest similarity that was included. Any similarities that are the same should thus also be included.\n",
    "# [4] refers to the above, while [1] extracts the similarity score from the fifth most similar word.\n",
    "top_similarity_w2v = top_five_list_w2v[4][1]\n",
    "top_one_same_w2v = top_five_w2v.filter(lambda x: x[1] == top_similarity_w2v).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccddac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Word2Vec for feature extraction, the top 5 users with similar interests were:\n",
      "user_id: 302864870, similarity score: 1.000\n",
      "user_id: 1214658729938997253, similarity score: 1.000\n",
      "user_id: 1382024523097239556, similarity score: 1.000\n",
      "user_id: 770655466590445568, similarity score: 1.000\n",
      "user_id: 25576953, similarity score: 1.000\n",
      "There were also 131 omitted entries with a similarity score of 1.000\n"
     ]
    }
   ],
   "source": [
    "# Returning results for W2V.\n",
    "print('Using Word2Vec for feature extraction, the top 5 users with similar interests were:')\n",
    "\n",
    "for i in range(5):\n",
    "    print('user_id: {}, similarity score: {:.3f}'.format(top_five_list_w2v[i][0], top_five_list_w2v[i][1]))\n",
    "\n",
    "if len(top_one_same_w2v) > 5:\n",
    "    print('There were also {} omitted entries with a similarity score of {:.3f}'.format(len(top_one_same_w2v)-5, top_similarity_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179041c",
   "metadata": {},
   "source": [
    "Currently takes just under 18 seconds to run on my machine. I believe the DataFrame -> RDD call (for mapping purposes) is not the most efficient way (I have included part of the code below), but due to time constraints I must stay with what works for now.\n",
    "\n",
    "There is also the possibility of using udfs on the DataFrame, but again I was unable to get those to work (as of week 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87de92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonyms = model.findSynonyms(input_user_features_w2v, 5)\n",
    "# for row in synonyms.collect():\n",
    "#     print(row)\n",
    "# # THIS SAVES A LOT OF TIME VS RDD\n",
    "# # UNABLE TO GET THIS TO WORK WITHOUT ITERATING THROUGH 'RESULT' TO FIND USER_IDs THAT MATCH THE WORD. ALSO HAVE TO CONSIDER WHEN TO END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6403fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for workload 1: 21.52614665031433\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print('Total time taken for workload 1: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17245602",
   "metadata": {},
   "source": [
    "# Workload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1a2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd5c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Playing around with concat_ws to disentangle the array of user_mentions.\n",
    "# For ease of splitting later on, as it creates an array of strings from array of bigint\n",
    "# No filtering as we need to give recommendations for every user.\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "                    SELECT user_id, concat_ws(\",\", user_mentions.id) AS user_mentions\n",
    "                    FROM tweets\n",
    "                    \"\"\")\n",
    "# df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3fcd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding above arrays, user mentions are split over rows if in array.\n",
    "df_split = df.withColumn(\"user_mention\", explode(split(df[\"user_mentions\"], \",\"))).drop(\"user_mentions\")\n",
    "\n",
    "# df_split.show(10, False)\n",
    "\n",
    "# Group array with count, thus creating the rating column we need.\n",
    "df_group = df_split.groupBy(\"user_id\", \"user_mention\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9667065",
   "metadata": {},
   "source": [
    "NB: There are user_ids who have not mentioned any other users, these have been left blank. For the purposes of converting from bigint to int (via indexing), as well as to pass onto the Collaborative Filtering framework, these nulls were converted to an index of 2 (denoting 3rd most common user_mention). I initially aimed to filter it out at the end, but it seems the recommendation system knew to remove them automatically, as it seems no one is ever suggested 2 in the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b0a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indexer parameters.\n",
    "# A pipeline method was used to index over both user_id and user_mention to ensure no OOV elements can occur during recommendation\n",
    "\n",
    "indexers = [StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_index\") ,\\\n",
    "            StringIndexer(inputCol=\"user_mention\", outputCol=\"user_mention_index\")]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "# Execute indexing\n",
    "df_index = pipeline.fit(df_group).transform(df_group).cache()\n",
    "\n",
    "# From https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379000bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast indexed columns to correct datatypes for Collaborative Filtering\n",
    "df_ratings = df_index.withColumn(\"user\", df_index[\"user_id_index\"].cast('int')).drop(\"user_id_index\")\\\n",
    "                    .withColumn(\"product\", df_index[\"user_mention_index\"].cast('int')).drop(\"user_mention_index\")\\\n",
    "                    .withColumn(\"rating\", df_index[\"count\"].cast('float')).drop(\"count\")\n",
    "\n",
    "\n",
    "# Defining parameters for ALS\n",
    "topx = 5\n",
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"user\", itemCol=\"product\", ratingCol=\"rating\")\n",
    "\n",
    "# Fit ALS\n",
    "model = als.fit(df_ratings)\n",
    "\n",
    "# Final Output\n",
    "# User Recommendations for all users, in indexed form. An example function is written below to return recommended\n",
    "# mention users. An alternative is to use a further mapping to restore all initial values.\n",
    "userRecs = model.recommendForAllUsers(topx).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d32533",
   "metadata": {},
   "source": [
    "For an input user_id, this sample code returns the top 5. There is a noticeable lag due to the index2user function that calls back to df_index.\n",
    "\n",
    "This ultimately solve the big_int issue via indexing but is a bit slow. I'm sure there is a better way, but due to time constraints this works for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe4c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 user recommendations for user 1205906586734125056 are:\n",
      "1: 14335586\n",
      "2: 254999238\n",
      "3: 40851610\n",
      "4: 23424533\n",
      "5: 22053725\n"
     ]
    }
   ],
   "source": [
    "# # Defining functions for user_id retrieval and index retrieval for final output\n",
    "# # Filter first then distinct was markedly quicker than the other way around.\n",
    "# def user2index(user_id):\n",
    "#     return df_index.select(\"user_id_index\").filter(df_index.user_id == user_id).distinct().collect()[0][0]\n",
    "\n",
    "# def index2user(index_id):\n",
    "#     return df_index.select(\"user_mention\").filter(df_index.user_mention_index == index_id).distinct().collect()[0][0]\n",
    "\n",
    "# def recommend2user(user_id):    \n",
    "#     user_index = user2index(user_id)\n",
    "#     user_rec = userRecs.filter(userRecs.user == user_index).collect()\n",
    "#     print('The top {} user recommendations for user {} are:'.format(topx, user_id))\n",
    "#     i = 1\n",
    "#     for row in user_rec[0][1]:\n",
    "#         print('{}: {}'.format(i, index2user(row['product'])))\n",
    "#         i += 1\n",
    "        \n",
    "# recommend2user(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32aca2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Collect top 5 mention indices and convert to userIds\n",
    "# def rec_mention(line):\n",
    "#     mentions = []\n",
    "#     user, recommended_mention_indices = line\n",
    "#     user = index2user(user)\n",
    "#     for i in range(len(recommended_mention_indices)):\n",
    "#         mention_index = recommended_mention_indices[i][0]\n",
    "#         mention = index2user(mention_index)\n",
    "#         mentions.append(mention)\n",
    "#     return (user, mentions)\n",
    "\n",
    "# userRecs.rdd.map(lambda row: rec_mention(row)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30a62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following RecommendForAllUsers from:\n",
    "# https://towardsdatascience.com/collaborative-filtering-in-pyspark-52617dd91194\n",
    "\n",
    "# TODO: Create rating column; need to aggregate by? DONE\n",
    "# should be able to pass above into the collaborative filtering algorithm for result DONE?\n",
    "# TODO: Run on EMR cluster\n",
    "# TODO: Report writeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a17e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting user_ids only (still indexed).\n",
    "user_rec = userRecs.select(\"user\", userRecs.recommendations[0][\"product\"].alias(\"Top 1\")\\\n",
    "                          , userRecs.recommendations[1][\"product\"].alias(\"Top 2\")\\\n",
    "                          , userRecs.recommendations[2][\"product\"].alias(\"Top 3\")\\\n",
    "                          , userRecs.recommendations[3][\"product\"].alias(\"Top 4\")\\\n",
    "                          , userRecs.recommendations[4][\"product\"].alias(\"Top 5\"))\n",
    "\n",
    "\n",
    "# Apply index to string to revert all indices to original format. I apologise in advance for how hardcoded this is.\n",
    "# Time-constraints and a goal of using as little python data structures as possible led to a bit of a mess.\n",
    "tmp = df_index.select(df_index.user_id_index.alias(\"user\"), df_index.user_id.alias(\"user_id\")).distinct()\n",
    "user_rec_id = user_rec.join(tmp, on=['user'], how = 'left').drop(\"user\").cache()\n",
    "\n",
    "tmp = df_index.select(df_index.user_mention_index.alias(\"Top 1\"), df_index.user_mention.alias(\"Top 1_id\")).distinct()\n",
    "user_rec_id = user_rec_id.join(tmp, on=['Top 1'], how = 'left').drop(\"Top 1\").cache()\n",
    "\n",
    "tmp = df_index.select(df_index.user_mention_index.alias(\"Top 2\"), df_index.user_mention.alias(\"Top 2_id\")).distinct()\n",
    "user_rec_id = user_rec_id.join(tmp, on=['Top 2'], how = 'left').drop(\"Top 2\").cache()\n",
    "\n",
    "tmp = df_index.select(df_index.user_mention_index.alias(\"Top 3\"), df_index.user_mention.alias(\"Top 3_id\")).distinct()\n",
    "user_rec_id = user_rec_id.join(tmp, on=['Top 3'], how = 'left').drop(\"Top 3\").cache()\n",
    "\n",
    "tmp = df_index.select(df_index.user_mention_index.alias(\"Top 4\"), df_index.user_mention.alias(\"Top 4_id\")).distinct()\n",
    "user_rec_id = user_rec_id.join(tmp, on=['Top 4'], how = 'left').drop(\"Top 4\").cache()\n",
    "\n",
    "tmp = df_index.select(df_index.user_mention_index.alias(\"Top 5\"), df_index.user_mention.alias(\"Top 5_id\")).distinct()\n",
    "user_rec_id = user_rec_id.join(tmp, on=['Top 5'], how = 'left').drop(\"Top 5\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77d5660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing first 50 rows of output:\n",
      "+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|user_id            |Top 1_id           |Top 2_id          |Top 3_id           |Top 4_id           |Top 5_id           |\n",
      "+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|1135586475024822275|14296273           |757303975         |198118653          |200737738          |45935021           |\n",
      "|250843319          |23922797           |370219796         |1078401427347857408|157981564          |114968487          |\n",
      "|2806767541         |15012486           |29780473          |24259259           |2704294333         |1353769946556325889|\n",
      "|422496868          |85583894           |33584794          |4970411            |65201417           |4207961            |\n",
      "|54579715           |17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|57243327           |3419251203         |991940041         |24261916           |2908786076         |138203134          |\n",
      "|738767160395321345 |15012486           |29780473          |24259259           |2704294333         |1353769946556325889|\n",
      "|1320221257804251136|15012486           |16973333          |153944899          |1321935792416149505|29780473           |\n",
      "|1138478460358275077|17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|19456591           |1643123766         |65201417          |14755475           |16467567           |865004396681207809 |\n",
      "|1083872734881505281|3240396234         |598921658         |134758540          |587591389          |28312814           |\n",
      "|1241563631621271552|17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|1244052031058391042|3240396234         |598921658         |134758540          |587591389          |28312814           |\n",
      "|760507592          |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|866949673461981184 |170291358          |94482117          |14296273           |2374745089         |59159771           |\n",
      "|1300923917201272836|17154865           |254999238         |1321935792416149505|854725669          |750073625915559936 |\n",
      "|66723691           |15012486           |2318270108        |626455898          |1124087157445005312|14434063           |\n",
      "|3178650079         |17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|547080803          |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|730523465447407616 |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|762115             |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|17393532           |2467791            |198118653         |9814812            |15859912           |85583894           |\n",
      "|428980372          |10228272           |90670300          |329866683          |1193633291506724864|3290364847         |\n",
      "|62977361           |3240396234         |55060090          |598921658          |24259259           |1321935792416149505|\n",
      "|715620316228292611 |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|1150613906563444737|14511951           |85583894          |198118653          |17112878           |41704988           |\n",
      "|1395310530         |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|2541571464         |3240396234         |598921658         |134758540          |587591389          |28312814           |\n",
      "|1360129634826817538|1155838084522618881|931571402         |1193633291506724864|598921658          |17445167           |\n",
      "|1276344823         |17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|1354272436397641729|17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|2703133905         |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|37793884           |17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|39763599           |14511951           |85583894          |198118653          |17112878           |41704988           |\n",
      "|78222755           |200737738          |55060090          |14296273           |842678603305377793 |320432933          |\n",
      "|18846626           |15012486           |29780473          |24259259           |2704294333         |1353769946556325889|\n",
      "|3815760209         |15012486           |29780473          |24259259           |2704294333         |1353769946556325889|\n",
      "|38886034           |1643123766         |33584794          |587591389          |17154865           |254999238          |\n",
      "|1214890524160610305|24259259           |17154865          |254999238          |854725669          |130557513          |\n",
      "|103150086          |17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|1063581671805587456|17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|183229409          |17154865           |789232424194572288|92555364           |1321935792416149505|252794509          |\n",
      "|2458425530         |17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|29422732           |17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|42953808           |3240396234         |55060090          |598921658          |24259259           |1321935792416149505|\n",
      "|874010558915162112 |15012486           |29780473          |24259259           |2704294333         |1353769946556325889|\n",
      "|1184641503261593601|17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|1211483960217178115|17154865           |254999238         |130557513          |854725669          |24259259           |\n",
      "|132339428          |15012486           |29780473          |24259259           |2704294333         |1353769946556325889|\n",
      "|170658171          |1643123766         |33584794          |587591389          |17154865           |254999238          |\n",
      "+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing final output.\n",
    "\n",
    "print(\"Printing first 50 rows of output:\")\n",
    "user_rec_id.show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fc964bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for workload 2: 11.753997802734375\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print('Total time taken for workload 2: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e631a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07e54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477356bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
